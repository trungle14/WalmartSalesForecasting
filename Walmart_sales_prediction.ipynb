{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00a03241"
      },
      "source": [
        "# Data download\n",
        "We have done feature engineering and saved the required data in google drive so that we will not have to repeat these steps again, and we use this data to run on google colab to test different models., also we are serializing the data frame directly using pickle instead of csv so that we donot need to define data types each time again and we can leverage the optimized data frames for algorithms"
      ],
      "id": "00a03241"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh8PEvyPOttC",
        "outputId": "850848ec-90bc-413a-ad2a-5f699dc607fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ccvbBiFHhlC9ZNLOBllGYBlMgxtUJ3T3\n",
            "To: /content/downloaded_file.zip\n",
            "100%|██████████| 2.20G/2.20G [00:23<00:00, 93.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'downloaded_file.zip' has been downloaded.\n"
          ]
        }
      ],
      "source": [
        "#https://drive.google.com/file/d/1ccvbBiFHhlC9ZNLOBllGYBlMgxtUJ3T3/view?usp=drive_link\n",
        "import gdown\n",
        "\n",
        "# Google Drive file ID\n",
        "file_id = '1ccvbBiFHhlC9ZNLOBllGYBlMgxtUJ3T3'\n",
        "\n",
        "# Destination file path where you want to save the downloaded file\n",
        "output_path = 'downloaded_file.zip'\n",
        "\n",
        "# Download the file using gdown\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "print(f\"File '{output_path}' has been downloaded.\")\n"
      ],
      "id": "bh8PEvyPOttC"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrijWJGyO-hr",
        "outputId": "5096e1d8-1ddb-4dd1-81b3-a3479487a361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  downloaded_file.zip\n",
            "replace input_data/enc_feats.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "! unzip downloaded_file.zip"
      ],
      "id": "HrijWJGyO-hr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below is the path where files are saved and model feeds on file here to make predictions"
      ],
      "metadata": {
        "id": "hVqUJ1rBRiy6"
      },
      "id": "hVqUJ1rBRiy6"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7D88esGPPKFI"
      },
      "outputs": [],
      "source": [
        "files_path = r'/content/input_data/'"
      ],
      "id": "7D88esGPPKFI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6mEFODbIeUZ"
      },
      "source": [
        "Inspired by popular solutions on kaggle, below code downloads necessary data sets from kaggle to google colab environment"
      ],
      "id": "c6mEFODbIeUZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a8a446f"
      },
      "source": [
        "### Utils and helpers for data pre processing and feature engineering"
      ],
      "id": "7a8a446f"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZrphmCNnxcIw"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import subprocess, psutil, os\n",
        "\n",
        "## Below is the regression libarary being used\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor\n",
        "from lightgbm.callback import early_stopping\n",
        "\n",
        "# since here x is in 2 dimensions we are leveraging tweedie distributions (we have noticed these features among best kaggle scores)\n",
        "\n",
        "lgb_params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'tweedie',\n",
        "    'tweedie_variance_power': 1.1,\n",
        "    'metric': 'rmse',\n",
        "    'subsample': 0.5,\n",
        "    'subsample_freq': 1,\n",
        "    'min_child_weight': 1,\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 2 ** 11 - 1,\n",
        "    'min_data_in_leaf': 2 ** 12 - 1,\n",
        "    'feature_fraction': 0.5,\n",
        "    'max_bin': 100,\n",
        "    'n_estimators': 1400,\n",
        "    'boost_from_average': False,\n",
        "    'verbosity': -1,\n",
        "    #'device': 'auto'\n",
        "}\n",
        "lgbm = LGBMRegressor(**lgb_params)\n",
        "callbacks = [early_stopping(stopping_rounds=50, first_metric_only=False)]"
      ],
      "id": "ZrphmCNnxcIw"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "83ba2579"
      },
      "outputs": [],
      "source": [
        "def sizeof_fmt(num, suffix='B'):\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "def reduce_mem_usage(dataframe, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_memory = dataframe.memory_usage().sum() / 1024**2\n",
        "    for column_selected in dataframe.columns:\n",
        "        column_type = dataframe[column_selected].dtypes\n",
        "        if column_type in numerics:\n",
        "            column_min = dataframe[column_selected].min()\n",
        "            column_max = dataframe[column_selected].max()\n",
        "            if str(column_type)[:3] == 'int':\n",
        "                if column_min > np.iinfo(np.int8).min and column_max < np.iinfo(np.int8).max:\n",
        "                    dataframe[column_selected] = dataframe[column_selected].astype(np.int8)\n",
        "                elif column_min > np.iinfo(np.int16).min and column_max < np.iinfo(np.int16).max:\n",
        "                    dataframe[column_selected] = dataframe[column_selected].astype(np.int16)\n",
        "                elif column_min > np.iinfo(np.int32).min and column_max < np.iinfo(np.int32).max:\n",
        "                    dataframe[column_selected] = dataframe[column_selected].astype(np.int32)\n",
        "                elif column_min > np.iinfo(np.int64).min and column_max < np.iinfo(np.int64).max:\n",
        "                    dataframe[column_selected] = dataframe[column_selected].astype(np.int64)\n",
        "            else:\n",
        "                if column_min > np.finfo(np.float16).min and column_max < np.finfo(np.float16).max:\n",
        "                    dataframe[column_selected] = dataframe[column_selected].astype(np.float16)\n",
        "                elif column_min > np.finfo(np.float32).min and column_max < np.finfo(np.float32).max:\n",
        "                    dataframe[column_selected] = dataframe[column_selected].astype(np.float32)\n",
        "                else:\n",
        "                    dataframe[column_selected] = dataframe[column_selected].astype(np.float64)\n",
        "    end_memory = dataframe.memory_usage().sum() / 1024**2\n",
        "    if verbose:\n",
        "        print(' Memory usage decreased by ({:.1f}% reduction)'.format(100 * (start_memory - end_memory) / start_memory))\n",
        "    return dataframe\n",
        "\n",
        "## Merging by concat to not lose optimized dtypes\n",
        "def merge_by_concat(df1, df2, merge_on):\n",
        "    merged_gf = df1[merge_on]\n",
        "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
        "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
        "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
        "    return df1\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "83ba2579"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f308cef"
      },
      "source": [
        "### Prices features\n",
        "\n",
        "so we are doing feature engineering here to get price related data, we have week wise data of price (we have price features for test weeks as well).\n",
        "\n",
        "We are using expanding max price , minimum price , standard deviation , mean, so that there is no data leakage from future to past, and ,model can solely use the past data.\n",
        "\n",
        "(since the data is already sorted time wise we are not sorting again , saves in computation time)."
      ],
      "id": "5f308cef"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9425df0d",
        "outputId": "ccb85414-add1-4daa-ffa2-e77777795e49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'prices.pkl' already exists. Skipping save operation.\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(f'{files_path}prices.pkl'):\n",
        "    print(\"The file 'prices.pkl' already exists. Skipping save operation.\")\n",
        "\n",
        "else:\n",
        "    print('Creating Price Features, adding max price, min price, std, mean, prev price')\n",
        "    price = pd.read_csv(f'{files_path}sell_prices.csv')\n",
        "    calendar = pd.read_csv(f'{files_path}calendar.csv')\n",
        "    calendar = reduce_mem_usage(calendar)\n",
        "    price = reduce_mem_usage(price)\n",
        "\n",
        "    print(f'Price Stats')\n",
        "    grp = price.groupby(['store_id','item_id'])['sell_price']\n",
        "    price['price_max'] = grp.transform(lambda x: x.expanding().max()).reset_index(drop=True)\n",
        "    price['price_min'] = grp.transform(lambda x: x.expanding().min()).reset_index(drop=True)\n",
        "    price['price_std'] = grp.transform(lambda x: x.expanding().std()).reset_index(drop=True)\n",
        "    price['price_mean'] = grp.transform(lambda x: x.expanding().mean()).reset_index(drop=True)\n",
        "    price['prev_sell_price'] = grp.transform(lambda x: x.shift(1))\n",
        "    del grp, calendar\n",
        "    price = reduce_mem_usage(price)\n",
        "    print(price.columns)\n",
        "    price.to_pickle(f'{files_path}prices.pkl')\n",
        "    del price"
      ],
      "id": "9425df0d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5f60170"
      },
      "source": [
        "### Base DataFrame\n",
        "\n",
        "We have data in wide format , but since we are leveraging on regression algorithms have modified to use data in long format, add rows for test data as well."
      ],
      "id": "f5f60170"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0f4a040",
        "outputId": "a6f1509a-826b-460c-8ab3-6d04a715770b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'base.pkl' already exists. Skipping save operation.\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(f'{files_path}base.pkl'):\n",
        "    print(\"The file 'base.pkl' already exists. Skipping save operation.\")\n",
        "\n",
        "else:\n",
        "    print(f'Transforming from wide to deep format. Some data types converstion for memory management')\n",
        "    TARGET = 'sales'\n",
        "    END_TRAIN = 1941\n",
        "    MAIN_INDEX = ['id','d']\n",
        "\n",
        "    eva = pd.read_csv(f'{files_path}sales_train_evaluation.csv')\n",
        "    print('Create Grid')\n",
        "    index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
        "    grid = pd.melt(eva,\n",
        "                      id_vars = index_columns,\n",
        "                      var_name = 'd',\n",
        "                      value_name = TARGET)\n",
        "\n",
        "    print(f'Train rows. Wide: {len(eva)}, Deep: {len(grid)}')\n",
        "\n",
        "    add_grid = pd.DataFrame()\n",
        "    for i in range(1,29):\n",
        "        temp_df = eva[index_columns]\n",
        "        temp_df = temp_df.drop_duplicates()\n",
        "        temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
        "        temp_df[TARGET] = np.nan\n",
        "        add_grid = pd.concat([add_grid,temp_df])\n",
        "\n",
        "    grid = pd.concat([grid,add_grid])\n",
        "    grid = grid.reset_index(drop=True)\n",
        "\n",
        "    del temp_df, add_grid, eva\n",
        "    print(\"{}: {}\".format('Original grid',sizeof_fmt(grid.memory_usage(index=True).sum())))\n",
        "\n",
        "    for col in index_columns:\n",
        "        grid[col] = grid[col].astype('category')\n",
        "\n",
        "    print(\"{}: {}\".format('Reduced grid',sizeof_fmt(grid.memory_usage(index=True).sum())))\n",
        "    grid = reduce_mem_usage(grid)\n",
        "    grid.to_pickle(f'{files_path}base.pkl')\n",
        "    del grid"
      ],
      "id": "c0f4a040"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2d96fd6"
      },
      "source": [
        "### Calendar Features\n",
        "\n",
        "we see prices of some items starting for a particular week, which might indicate that would be release week for the product so we can use data in base data frame after that point (as since earlier data was in long format it would have data for all items through all days)\n",
        " This reduces size of the data and will have feature of when the product was released (capturing any trends if item get sold when we are predicting for volumes closer to release dates)\n",
        "\n",
        "Then we do label encoding of the categorical features so that they can be used for regression algorithms\n"
      ],
      "id": "a2d96fd6"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0b17d94",
        "outputId": "ce43c8f2-bf0c-4094-de71-4f847238c679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'no_feat.pkl' already exists. Skipping save operation.\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(f'{files_path}no_feat.pkl'):\n",
        "    print(\"The file 'no_feat.pkl' already exists. Skipping save operation.\")\n",
        "\n",
        "else:\n",
        "    print('Creating release week and removing rows before price data exists. Adding price features.')\n",
        "    price = pd.read_csv(f'{files_path}sell_prices.csv')\n",
        "    calendar = pd.read_csv(f'{files_path}calendar.csv')\n",
        "\n",
        "    release_df = price.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
        "    release_df.columns = ['store_id','item_id','release']\n",
        "\n",
        "    grid = pd.read_pickle(f'{files_path}base.pkl')\n",
        "    grid = merge_by_concat(grid, release_df, ['store_id','item_id'])\n",
        "    del release_df\n",
        "\n",
        "    grid = merge_by_concat(grid, calendar[['wm_yr_wk','d']], ['d'])\n",
        "    grid = grid[grid['wm_yr_wk']>=grid['release']]\n",
        "    grid = grid.reset_index(drop=True)\n",
        "    grid = reduce_mem_usage(grid)\n",
        "\n",
        "    grid = merge_by_concat(grid, price, ['store_id','item_id','wm_yr_wk'])\n",
        "    grid = reduce_mem_usage(grid)\n",
        "    print(grid.columns)\n",
        "    del price, calendar\n",
        "    grid['release'] = grid['release'] - grid['release'].min()\n",
        "    grid['release'] = grid['release'].astype(np.int16)\n",
        "\n",
        "    price = pd.read_pickle(f'{files_path}prices.pkl')\n",
        "    grid = grid.merge(price.drop(['sell_price'], axis=1), on = ['store_id','item_id','wm_yr_wk'], how='left')\n",
        "\n",
        "    calendar = pd.read_csv(f'{files_path}calendar.csv')\n",
        "    grid = grid.merge(calendar.drop(['weekday','year','wday','month','wm_yr_wk'], axis=1), on = ['d'], how = 'left')\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    cat_vars = ['item_id','store_id','dept_id','cat_id','state_id','event_name_1','event_type_1','event_name_2','event_type_2']\n",
        "    del price, calendar\n",
        "    for cat in cat_vars:\n",
        "        grid[cat] = le.fit_transform(grid[cat])\n",
        "\n",
        "    grid['date'] = grid['date'].astype('datetime64[ns]')\n",
        "    grid['tm_d'] = grid['date'].dt.day.astype(np.int8)\n",
        "    grid['tm_w'] = grid['date'].dt.isocalendar().week.astype(np.int8)\n",
        "    grid['tm_m'] = grid['date'].dt.month.astype(np.int8)\n",
        "    grid['tm_y'] = grid['date'].dt.year\n",
        "    grid['tm_y'] = (grid['tm_y'] - grid['tm_y'].min()).astype(np.int8)\n",
        "    grid['tm_dw'] = grid['date'].dt.dayofweek.astype(np.int8)\n",
        "    grid['tm_w_end'] = (grid['tm_dw'] >= 5).astype(np.int8)\n",
        "    grid['d'] = grid['d'].str.replace('d_', '').astype('int16')\n",
        "    grid = reduce_mem_usage(grid)\n",
        "    grid.to_pickle(f'{files_path}no_feat.pkl')\n",
        "    del grid"
      ],
      "id": "c0b17d94"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a82e31c"
      },
      "source": [
        "### Lag and rolling lags features\n",
        "\n",
        "Another important feature we observed in winning solutions is they used lags data and roll data in feature engineering, this kind of gives how trends data could be captured using regression algorithm , though we are not specifically using time series data.\n",
        "\n",
        "for this we have considered rolling sum of number of times, 0 units of product were sold, 7, 14, 30, 60, 180 days of roll (week, 2 weeks, approx month, 2 months approx, approx half year), with this we will be able to acpture trend details.\n",
        "\n",
        "as next important features we have chosen lag features (these will capture sales with a lag of that manay days we have in feature."
      ],
      "id": "4a82e31c"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff20ac56",
        "outputId": "4079924a-7485-4e26-ce77-a806ae3221eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'lags.pkl' already exists. Skipping save operation.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if os.path.exists(f'{files_path}lags.pkl'):\n",
        "    print(\"The file 'lags.pkl' already exists. Skipping save operation.\")\n",
        "\n",
        "else:\n",
        "    print(f'Creating a variety lag features')\n",
        "    grid = pd.read_pickle(f'{files_path}no_feat.pkl')\n",
        "    grid = grid[['id','d','sales']]\n",
        "\n",
        "    zero_grid = grid.loc[:,['id','sales']]\n",
        "    zero_grid['is_zero'] = (grid['sales'] == 0).astype(int)\n",
        "    zero_grid = zero_grid.drop(['sales'], axis=1)\n",
        "    grid['is_zero'] = zero_grid['is_zero']\n",
        "\n",
        "    grp = grid.groupby(['id'], group_keys=False, observed=False)['sales']\n",
        "    grp_z = grid.groupby(['id'], group_keys=False, observed=False)['is_zero']\n",
        "\n",
        "    grid = reduce_mem_usage(grid)\n",
        "    print('************ ROLLING LAGS ************')\n",
        "    for roll in [7, 14, 30, 60, 180]:\n",
        "        grid[f'rolling_zero_{roll}'] = grp_z.transform(lambda x: x.rolling(roll).sum())\n",
        "        grid[f'rm_{roll}'] = grp.transform(lambda x: x.rolling(roll).mean())\n",
        "        grid[f'std_{roll}'] = grp.transform(lambda x: x.rolling(roll).std())\n",
        "        grid[f'diff_rm_{roll}'] = grp.transform(lambda x : x.diff().rolling(roll).mean())\n",
        "        grid[f'max_{roll}'] = grp.transform(lambda x: x.rolling(roll).max())\n",
        "        grid = reduce_mem_usage(grid)\n",
        "\n",
        "    del zero_grid\n",
        "\n",
        "    grid = reduce_mem_usage(grid)\n",
        "    print('************ LAGS ************')\n",
        "    for lag in np.arange(0, 15, 1):\n",
        "        grid[f'lag_{lag}'] = grp.transform(lambda x: x.shift(lag))\n",
        "\n",
        "    grid = grid.drop(['is_zero', 'sales'], axis = 1)\n",
        "    ix_to_drop = grid[(grid['d'] <= 1941) & (grid.isna().any(axis=1))].index\n",
        "    grid.drop(index=ix_to_drop, inplace=True)\n",
        "    grid = reduce_mem_usage(grid)\n",
        "    grid.to_pickle(f'{files_path}lags.pkl')\n",
        "    del grid"
      ],
      "id": "ff20ac56"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3b43dd6"
      },
      "source": [
        "### Categorical Encodings\n",
        "\n",
        "We then use category wise sales data, item wise sales data, department wise sales data (across all stores), then also use store and category wise sales data, store and item wise sales data, store and department wise sales data.\n",
        "\n",
        "This gives cross sectional features that our model could pick if there is any trend."
      ],
      "id": "f3b43dd6"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9586ace",
        "outputId": "da579b46-eb8c-41c3-c52d-528a420b2616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'enc_feats.pkl' already exists. Skipping save operation.\n"
          ]
        }
      ],
      "source": [
        "# LAGs DATAFRAME - Only needs to be run once.\n",
        "if os.path.exists(f'{files_path}enc_feats.pkl'):\n",
        "    print(\"The file 'enc_feats.pkl' already exists. Skipping save operation.\")\n",
        "\n",
        "else:\n",
        "    print(f'Creating categorical feature encodings')\n",
        "    grid = pd.read_pickle(f'{files_path}no_feat.pkl')\n",
        "    grid = grid[['id','d','sales','item_id','dept_id','cat_id','store_id','state_id']]\n",
        "    grid = reduce_mem_usage(grid)\n",
        "    for col_name in ['cat_id', 'item_id', 'dept_id', 'store_id', 'store_id,cat_id', 'store_id,item_id', 'store_id,dept_id']:\n",
        "        col_names = col_name.split(',')\n",
        "        s_col_name = col_name.replace(',', '_')\n",
        "        grid[f'{s_col_name}_enc'] = grid.groupby(col_names, observed=False)['sales'].transform(lambda x: x.expanding().mean())\n",
        "\n",
        "    print('************ CATEGORIES ENCODED ************')\n",
        "    # Memory reduction\n",
        "    grid = grid.drop(['sales','item_id','dept_id','cat_id','store_id','state_id'], axis=1)\n",
        "    grid = reduce_mem_usage(grid)\n",
        "    grid.to_pickle(f'{files_path}enc_feats.pkl')\n",
        "    del grid"
      ],
      "id": "e9586ace"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d6bbd13"
      },
      "source": [
        "### Set up"
      ],
      "id": "9d6bbd13"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be6a91c9"
      },
      "source": [
        "### Train and Predict\n",
        "\n",
        "First we need to model comparision to see which model produces better kaggle score and use that model , then optimize the step size so as to improve the score further.\n",
        "\n",
        "through this process we are basically using the sales data that we have on t- step (here for 1- 14 days prediction step  will be 14, and from 14 till 28th day step will be 28 days)\n",
        "\n",
        "Then we run prediction model where we first loop over store and department to train the model (slowe) , next over store and category (will be quicker) and take average of both the methods to arrive at final submission\n"
      ],
      "id": "be6a91c9"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "45e3818b"
      },
      "outputs": [],
      "source": [
        "horizon = 28\n",
        "base = pd.read_pickle(f'{files_path}no_feat.pkl')\n",
        "\n",
        "STEPS = [14, 28]\n",
        "TARGET = ['sales']\n",
        "VAL_DAYS, TEST_DAYS = STEPS[0], STEPS[0]\n",
        "STORES = base.store_id.unique()\n",
        "DEPTS = base.dept_id.unique()\n",
        "CATS = base.cat_id.unique()\n",
        "ITEMS = base.item_id.unique()\n",
        "\n",
        "train_start = 1\n",
        "train_end = 1941 - horizon\n",
        "first_val_day = train_end + 1\n",
        "last_val_day = 1941\n",
        "first_pred_day = 1941 + 1\n",
        "val_start = 1942-28\n",
        "val_end = 1941\n",
        "\n",
        "feats = pd.read_pickle(f'{files_path}lags.pkl')\n",
        "enc_feats = pd.read_pickle(f'{files_path}enc_feats.pkl')\n",
        "\n",
        "remove_colums = ['id','item_id','dept_id','cat_id','store_id','state_id','d','sales','wm_yr_wk','date']\n",
        "enc_columns = enc_feats.columns[2:]\n",
        "lags_columns = list(feats.columns[2:]) + list(enc_columns)\n",
        "train_columns = list(base.columns[~base.columns.isin(remove_colums)]) + list(lags_columns)\n",
        "\n",
        "del base, feats, enc_feats"
      ],
      "id": "45e3818b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac040f3a",
        "outputId": "f3f94b4b-e255-42e5-9a3f-4bb535657e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************ Training Store 1 ************\n",
            "************ department 3 ************\n",
            "Val start: 1914. Val end 1941. Pred start 1942 Pred end 1955\n",
            "Train shape: (571079, 67). Val shape: (11648, 67). Test shape: (5824, 67)\n",
            "Training until validation scores don't improve for 50 rounds\n"
          ]
        }
      ],
      "source": [
        "predictions = pd.DataFrame()\n",
        "\n",
        "for store in STORES:\n",
        "    print(f'************ Training Store {store+1} ************')\n",
        "    for dept in DEPTS:\n",
        "          print(f'************ department {dept} ************')\n",
        "          for step in STEPS:\n",
        "                base = pd.read_pickle(f'{files_path}no_feat.pkl')\n",
        "                base = base[(base['dept_id']==dept) & (base['store_id']==store)]\n",
        "                store_ids = base.id.unique()\n",
        "                feats = pd.read_pickle(f'{files_path}lags.pkl')\n",
        "                feats = feats[feats['id'].isin(store_ids)]\n",
        "                enc_feats = pd.read_pickle(f'{files_path}enc_feats.pkl')\n",
        "                enc_feats = enc_feats[enc_feats['id'].isin(store_ids)]\n",
        "                grid = base.merge(feats, on=['id', 'd'], how='left')\n",
        "                del feats, base\n",
        "                grid = grid.merge(enc_feats, on=['id', 'd'])\n",
        "                del enc_feats\n",
        "                grid[lags_columns] = grid.groupby(['id'], observed=False)[lags_columns].shift(step)\n",
        "                ix_to_drop = grid[(grid['d'] <= 1941) & grid.isna().any(axis=1)].index\n",
        "                grid.drop(index=ix_to_drop, inplace=True)\n",
        "\n",
        "                pred_start = first_pred_day + step - VAL_DAYS\n",
        "                pred_end = first_pred_day + step - 1\n",
        "                print(f'Val start: {val_start}. Val end {val_end}. Pred start {pred_start} Pred end {pred_end}')\n",
        "                trainX = grid[(grid['d'] >= train_start) & (grid['d'] <= train_end)][train_columns]\n",
        "                trainY = grid[(grid['d'] >= train_start) & (grid['d'] <= train_end)][TARGET]\n",
        "                valX = grid[(grid['d'] >= val_start) & (grid['d'] <= val_end)][train_columns]\n",
        "                valY = grid[(grid['d'] >= val_start) & (grid['d'] <= val_end)][TARGET]\n",
        "                testX = grid[(grid['d'] >= pred_start) & (grid['d'] <= pred_end)][train_columns]\n",
        "                print(f'Train shape: {trainX.shape}. Val shape: {valX.shape}. Test shape: {testX.shape}')\n",
        "\n",
        "                # Train\n",
        "                lgbm.fit(trainX, trainY,\n",
        "                        eval_set=[(valX, valY)],\n",
        "                        eval_metric='rmse',\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "                 # Predict\n",
        "                yhat = lgbm.predict(testX, num_iteration=lgbm.best_iteration_)\n",
        "                preds = grid[(grid['d'] >= pred_start) & (grid['d'] <= pred_end)][['id', 'd']]\n",
        "                preds['sales'] = yhat\n",
        "                predictions = pd.concat([predictions, preds], axis=0)\n",
        "\n",
        "predictions.to_pickle(f'{files_path}store_depts_preds.pkl')\n",
        "del predictions"
      ],
      "id": "ac040f3a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3acfb07"
      },
      "outputs": [],
      "source": [
        "predictions = pd.DataFrame()\n",
        "\n",
        "for store in STORES:\n",
        "    print(f'************ Training Store {store+1} ************')\n",
        "    for cat in CATS:\n",
        "          print(f'************ category {cat} ************')\n",
        "          for step in STEPS:\n",
        "                base = pd.read_pickle(f'{files_path}no_feat.pkl')\n",
        "                base = base[(base['cat_id']==cat) & (base['store_id']==store)]\n",
        "                store_ids = base.id.unique()\n",
        "                feats = pd.read_pickle(f'{files_path}lags.pkl')\n",
        "                feats = feats[feats['id'].isin(store_ids)]\n",
        "                enc_feats = pd.read_pickle(f'{files_path}enc_feats.pkl')\n",
        "                enc_feats = enc_feats[enc_feats['id'].isin(store_ids)]\n",
        "                grid = base.merge(feats, on=['id', 'd'], how='left')\n",
        "                del feats, base\n",
        "                grid = grid.merge(enc_feats, on=['id', 'd'])\n",
        "                del enc_feats\n",
        "                grid[lags_columns] = grid.groupby(['id'], observed=False)[lags_columns].shift(step)\n",
        "                ix_to_drop = grid[(grid['d'] <= 1941) & grid.isna().any(axis=1)].index\n",
        "                grid.drop(index=ix_to_drop, inplace=True)\n",
        "\n",
        "                pred_start = first_pred_day + step - VAL_DAYS\n",
        "                pred_end = first_pred_day + step - 1\n",
        "                print(f'Val start: {val_start}. Val end {val_end}. Pred start {pred_start} Pred end {pred_end}')\n",
        "                trainX = grid[(grid['d'] >= train_start) & (grid['d'] <= train_end)][train_columns]\n",
        "                trainY = grid[(grid['d'] >= train_start) & (grid['d'] <= train_end)][TARGET]\n",
        "                valX = grid[(grid['d'] >= val_start) & (grid['d'] <= val_end)][train_columns]\n",
        "                valY = grid[(grid['d'] >= val_start) & (grid['d'] <= val_end)][TARGET]\n",
        "                testX = grid[(grid['d'] >= pred_start) & (grid['d'] <= pred_end)][train_columns]\n",
        "                print(f'Train shape: {trainX.shape}. Val shape: {valX.shape}. Test shape: {testX.shape}')\n",
        "\n",
        "\n",
        "                #training\n",
        "                lgbm.fit(trainX, trainY,\n",
        "                        eval_set=[(valX, valY)],\n",
        "                        eval_metric='rmse',\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "                # Predict\n",
        "                yhat = lgbm.predict(testX, num_iteration=lgbm.best_iteration_)\n",
        "                preds = grid[(grid['d'] >= pred_start) & (grid['d'] <= pred_end)][['id', 'd']]\n",
        "                preds['sales'] = yhat\n",
        "                predictions = pd.concat([predictions, preds], axis=0)\n",
        "\n",
        "predictions.to_pickle(f'{files_path}store_cats_preds.pkl')\n",
        "del predictions"
      ],
      "id": "c3acfb07"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b10d212"
      },
      "source": [
        "### Submission"
      ],
      "id": "7b10d212"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2150e9fa"
      },
      "outputs": [],
      "source": [
        "pred1 = pd.read_pickle(f'{files_path}store_depts_preds.pkl')\n",
        "pred2 = pd.read_pickle(f'{files_path}store_cats_preds.pkl')\n",
        "\n",
        "\n",
        "pred1.set_index(['id', 'd'], inplace=True)\n",
        "pred2.set_index(['id', 'd'], inplace=True)\n",
        "\n",
        "df_avg = (pred1 + pred2) / 2\n",
        "df_avg.reset_index(inplace=True)\n",
        "\n",
        "submission = pd.read_csv(f'{files_path}sample_submission.csv')\n",
        "df_avg = df_avg.pivot(index='id', columns='d', values='sales').reset_index()\n",
        "df_avg.columns = submission.columns\n",
        "df_avg = submission[['id']].merge(df_avg, on='id', how='left').fillna(1)\n",
        "print(df_avg)\n",
        "submission_file = f\"{files_path}submission_svr.csv\"\n",
        "df_avg.to_csv(f'{submission_file}', index=False)\n"
      ],
      "id": "2150e9fa"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 40269.483753,
      "end_time": "2023-10-30T00:13:07.891234",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-10-29T13:01:58.407481",
      "version": "2.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}